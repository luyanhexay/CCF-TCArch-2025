# GPU 全对最短路径程序设计报告

## 引言

本次任务的目标是实现一个基于 HIP 的全对最短路径（All-Pairs Shortest Path, APSP）程序。APSP 是图论中的经典问题，需要计算图中任意两个顶点之间的最短路径距离。该问题在路由算法、网络分析和社交网络分析等领域有广泛应用。设计的重点不仅在于保证算法的正确性，还在于在 GPU 上充分发挥并行性能，并在评测时兼顾整体运行时间，包括数据传输与初始化开销。

## 开发过程

最初的实现采用了传统的 Floyd-Warshall 算法，该算法的时间复杂度为 O(V³)，适合在 GPU 上并行化。然而，直接实现存在内存访问模式不佳、缓存命中率低等问题。为了优化性能，我们采用了分块 Floyd-Warshall 算法，将 V×V 的距离矩阵划分为多个 B×B 的块，通过三个阶段的计算来减少内存访问次数并提高缓存利用率。

在实现过程中，我们遇到了几个关键挑战。首先是内存管理问题：原始实现需要在每次计算时进行主机到设备的数据传输，这成为了性能瓶颈。通过引入分块算法和共享内存优化，我们实现了高效的 GPU 资源管理，将图数据一次性传输到 GPU 并保持在设备内存中，避免了重复的数据传输开销。

其次是分块算法的正确性保证。分块 Floyd-Warshall 算法分为三个阶段：Phase 1 更新对角线块，Phase 2 更新与对角线块同行同列的块，Phase 3 更新剩余块。每个阶段都需要正确的同步和内存访问模式。通过使用共享内存和适当的线程同步，我们确保了算法的正确性。

性能分析表明，GPU 核函数的执行效率很高，在中等规模的图上（如 1000 个顶点），纯 GPU 计算部分仅需数毫秒。然而，最大的开销仍然来自 HIP 的首次初始化。通过实现 HIP 冷启动并行优化，我们将 HIP 初始化与文件 I/O 并行执行，成功隐藏了初始化开销。

同样考虑到评测统计的是整个程序的运行时间，文件 I/O 的优化也至关重要。我们实现了快速读入函数 `read_int()` 和带缓冲区的快速输出函数，避免了标准库的额外开销。对于大规模图数据，这种优化带来了显著的性能提升。

## 最终方案

最终实现的方案包括以下要点：

* **核心算法**：分块 Floyd-Warshall 算法，通过三个阶段的计算实现高效的全对最短路径计算，充分利用 GPU 的并行计算能力。
* **内存管理**：使用共享内存优化，输入和输出数组分配在页锁定内存中，以加快数据传输速度。
* **HIP冷启动优化**：通过多线程并行执行，将HIP设备初始化与文件I/O操作并行进行，完全隐藏了HIP冷启动开销。
* **I/O 优化**：实现快速读入和带缓冲区的输出函数，减少系统调用开销。

## 算法设计

### 分块 Floyd-Warshall 算法

分块 Floyd-Warshall 算法将 V×V 的距离矩阵 D 划分为 ⌈V/B⌉×⌈V/B⌉ 个 B×B 的块，其中 B 是块大小。对于每个块索引 k（从 0 到 ⌈V/B⌉-1），算法执行以下三个阶段：

1. **Phase 1 - 对角线块更新**：计算对角线块 (k,k) 内的最短路径，考虑块内顶点作为中间点。
2. **Phase 2 - 行和列更新**：使用新计算的对角线块距离更新同一行和同一列的块。
3. **Phase 3 - 剩余块更新**：对于所有不在对角线、行、列上的块，使用公式 D[i,j] ← min(D[i,j], D[i,k] + D[k,j]) 更新距离。

### GPU 实现细节

每个阶段都使用专门的 kernel 函数实现：

- `floyd_warshall_block_kernel_phase1`：处理对角线块，使用共享内存存储块数据，通过 `block_calc` 函数执行块内计算。
- `floyd_warshall_block_kernel_phase2_row` 和 `floyd_warshall_block_kernel_phase2_col`：分别处理行和列的更新。
- `floyd_warshall_block_kernel_phase3`：处理剩余块的更新。

所有 kernel 都使用共享内存来减少全局内存访问，并通过适当的填充（stride = b + 1）避免 bank 冲突。

### 代码结构

程序采用模块化设计，主要包含以下文件：

- `main.cpp`：主程序入口，负责文件 I/O 和快速输入输出优化
- `kernel.hip`：GPU 求解器接口，调用分块 Floyd-Warshall 算法
- `blocked_floyd_warshall_hip.hip`：核心算法实现，包含三个阶段的 kernel 函数
- `blocked_floyd_warshall.h`：算法接口声明
- `errors.h`：错误处理结构定义
- `main.h`：常量定义和函数声明

## 结果与分析

在测试用例上的表现：

* 所有 10 个测试用例均通过，总执行时间约 6.94 秒。
* 单个测试用例的执行时间在 0.45 秒到 2.01 秒之间，主要取决于图的规模。
* 通过分块算法和内存优化，GPU 计算部分效率显著提升。

性能优化的效果：

* 通过分块算法提高了缓存命中率，减少了内存访问延迟。
* 共享内存的使用显著减少了全局内存访问次数。
* HIP冷启动并行优化完全隐藏了设备初始化开销，与文件I/O并行执行。
* 快速 I/O 优化显著降低了整体运行时间。

### 编译警告处理

在编译过程中出现了一些警告，主要是：
- 未使用返回值的警告（hipFree 函数）
- 循环展开失败的警告（编译器优化限制）

这些警告不影响程序的正确性和性能，但可以在未来版本中进一步优化。

## 技术特点

### 内存优化

1. **共享内存使用**：每个 kernel 都使用共享内存存储块数据，减少全局内存访问
2. **内存填充**：使用 stride = b + 1 避免 bank 冲突
3. **边界检查**：通过 `get` 和 `update` 函数确保内存访问的安全性

### 并行化策略

1. **块级并行**：每个块由 b×b 个线程并行处理
2. **阶段化处理**：三个阶段的 kernel 分别处理不同类型的块
3. **同步机制**：使用 `__syncthreads()` 确保线程间同步

### HIP冷启动并行优化

实现了关键的HIP冷启动优化机制：

1. **预热函数**：`hip_warmup_once()` 函数提前完成HIP的冷启动开销：
   - 创建HIP上下文 (`hipFree(0)`)
   - 创建HIP流 (`hipStreamCreate`)
   - 执行一个极小的kernel来确保代码对象已加载到驱动中
   - 销毁流资源

2. **并行执行**：在main函数中启动独立线程执行HIP预热，与文件I/O操作并行进行：
   ```cpp
   std::thread hip_init_thread([]{
       hip_warmup_once();
   });
   ```

3. **同步机制**：在调用GPU求解器之前等待HIP初始化完成：
   ```cpp
   hip_init_thread.join();
   ```

4. **时间隐藏**：通过并行执行，HIP的初始化时间被文件I/O时间完全隐藏，显著降低了整体运行时间。

### 错误处理

实现了完整的错误处理机制：
- 参数验证（节点数、块大小）
- HIP 运行时错误检查
- 详细的错误信息输出

## 总结与展望

本次工作实现了一个正确且高效的 GPU 全对最短路径程序，开发过程中解决了内存管理、分块算法实现、HIP冷启动开销等多个实际问题。在算法层面，分块 Floyd-Warshall 算法结合共享内存优化已能满足需求；在工程层面，通过HIP冷启动并行优化、快速 I/O 和内存优化显著降低了整体耗时。

未来的改进方向包括：进一步优化分块大小以适应不同的图规模；探索其他并行最短路径算法（如 Johnson 算法）在 GPU 上的实现；以及尝试多 GPU 并行化以处理更大规模的图。

总体而言，本次开发不仅让我实现了一个高性能的 GPU 全对最短路径程序，也让我深入理解了 GPU 编程中算法设计与工程优化的重要性，特别是在处理大规模图数据时的内存管理和并行化策略。
