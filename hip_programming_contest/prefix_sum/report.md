# GPU 前缀和程序设计报告

## 引言

本次任务的目标是实现一个基于 HIP 的整数前缀和程序。前缀和（Prefix Sum/Scan）是并行计算中的经典问题，广泛应用于数据处理和图形学等场景。设计的重点不仅在于保证算法的正确性，还在于在 GPU 上充分发挥并行性能，并在评测时兼顾整体运行时间，包括数据传输与初始化开销。

## 开发过程

最初的实现采用了传统的二段式思路：在 GPU 上进行块内扫描以得到局部前缀和和块总和，再将所有块的总和传回 CPU 做一次 exclusive 扫描，随后将偏移量传回 GPU，并通过一个 kernel 完成 uniform add。该方案在理论上可行，但在实际运行中暴露出若干问题。早期版本在 inclusive 与 exclusive 的界限上经常出错，导致结果整体偏移一个元素；在条带式索引下未能正确处理“更早列”的贡献，从而在跨块时出现错误；在小规模输入时甚至出现了错误的全零输出。

在逐步调试的过程中，块内扫描的实现方式经历了从基础的 Blelloch 扫描到条带式 warp 优化的演变。条带式的设计初衷是提高访存合并度，但一开始缺少了对“更早列总和”的补偿，导致从第二个 block 起结果出现明显错误。通过引入 `col_prefix` 来累计之前列的总和，并在每列计算时正确补上这一部分，问题得到解决。同时，在关键步骤之间增加必要的同步屏障，避免不同线程对共享变量 `col_prefix` 的读写出现竞争，从而解决了偶发性的局部错误。

性能分析表明，GPU 核函数的耗时极低，在一百万的数据规模下，单次运行仅需 1 毫秒左右，加上主机到设备与设备到主机的数据传输，总时间也仅在数毫秒的量级。然而，最大的开销来自 HIP 的首次初始化，这使得即使经过前面的分析，GPU部分的总时间仅在数毫秒，但整个`solve`函数的运行时间达到了三百多毫秒。由于评测统计的是整个程序的运行时间，这一初始化代价必须被正视。最终的优化方案是将 HIP 的初始化提前到 `main` 函数，并通过单独线程与文件 I/O 并行执行。这样一来，初始化开销被 I/O 掩盖，总体运行时间大幅下降。

同样考虑到，本次实验测试的是整个程序的运行时间，所以文件IO的开销也需要减少。官方原始提供的框架利用fstream库进行文件读写，使用iostream标准库进行输出，这两部分占用了大部分的运行时间。因为我们的读入数据是定好的int，在队长的帮助下，直接按字节读写，手动解析数字，并且建立write缓冲区，减少系统调用，从而提升性能。对于大数据样本的提升较为明显，在两千万的数据规模下，仅仅是IO的调整就让运行时间下降了两秒多。

## 最终方案

最终实现的方案包括以下要点：

* **核心算法**：条带式 warp 优化的块内扫描，结合两段式的块间偏移处理，保证了正确性和较高的执行效率。
* **内存管理**：输入和输出数组分配在 pinned 内存中，以加快数据传输速度。
* **初始化优化**：HIP 初始化通过后台线程提前完成，与文件读取并行，从而消除了对整体耗时的影响。
* **计时分析**：在 `solve` 内部引入精细化的计时机制，明确区分 GPU 核函数、数据传输、块间偏移和主机操作的耗时来源。

## 结果与分析

在 100 万规模的数据下：

* GPU 核函数耗时约 0.6 ms；
* H2D 与 D2H 数据传输共约 3.5 ms；
* 块间扫描与 uniform add 耗时不到 0.1 ms；
* 整个 `solve` 的实际 GPU 部分耗时 < 10 ms。

若不做初始化优化，总耗时约 330 ms，其中 300 ms 来自 HIP 冷启动。经过并行初始化优化后，总时间下降到约 10 ms，性能表现达到预期目标。

## 总结与展望

本次工作实现了一个正确且高效的 GPU 前缀和程序，开发过程中解决了索引映射、同步细节、初始化开销等多个实际问题。在算法层面，条带式 warp 优化结合两段式块间处理已能满足需求；在工程层面，通过 pinned 内存和初始化并行化显著降低了整体耗时。

未来的改进方向包括：进一步探索单 kernel 的 Decoupled Look-Back (DLB) 算法，以完全消除块间 CPU 参与；以及尝试向量化访存方式（如 int2/int4），进一步提升带宽利用率。

总体而言，本次开发不仅让我实现了一个高性能的 GPU 前缀和程序，也让我认识到 GPU 编程中算法设计与工程优化的重要性。
